{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d63d3f",
   "metadata": {},
   "source": [
    "## 1. In the sense of machine learning, what is a model? What is the best way to train a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86ce68",
   "metadata": {},
   "source": [
    "A machine learning model is a file which has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data.\n",
    "\n",
    "There is no best way to train a model. It's all about the experiments which we perform for optimizing the parameters of a certain algorithm and also performing hyperparameter tuning. Some tasks which we can do for making the model more general in nature is to perform:\n",
    "\n",
    "* Cross-validation techniques\n",
    "* hyper-parameter optimization etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6e826",
   "metadata": {},
   "source": [
    "## 2. In the sense of machine learning, explain the \"No Free Lunch\" theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee963eed",
   "metadata": {},
   "source": [
    "In a famous 1996 paper, David Wolpert demonstrated that if we make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the *No free Lunch* (NFL) theorem. For some datasets the best model is a linear model, while for other datasets it is a neural network. There is no model that is *a priori* guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice we make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks we may evaluate linear models with various levels of regularization, and for a complex problem we may evaluate various neural networks.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c890d",
   "metadata": {},
   "source": [
    "## 3. Describe the K-fold cross-validation mechanism in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71373df",
   "metadata": {},
   "source": [
    "In K-fold cross-validation, we randomly split the training dataset into k folds without replacement. Here, k-1 folds, the so-called training folds, are used for the model training, and one fold, the so-called test fold, is used for performance evaluation. This procedure is repeated k-times so that we obtain k models and performance estimates. We then calculate the average performance of the models based on the different, independent test folds to obtain a performance estimate that is less sensitive to the sub-partitioning of the training data compared to the holdout method. Typically, we use k-fold cross-validation for model tuning, that is, finding the optimal hyperparameter values that yield a satisfying generalization performance, which is estimated from evaluating the model performance on the test folds.\n",
    "\n",
    "Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training dataset and obtain a final performance estimate using the independent test dataset. The rationale behind fitting a model to the whole training dataset after k-fold cross-validation is that first, we are typically ineterested in a single, final model (versus k individual models), and second, providing more training examples to a learning algorithm usually results in a more accurate and robust model. Suppose k = 10, the training dataset is divided into 10 folds, and during the 10 iterations, 9 folds are used for training, and 1 fold will be used as the test dataset for model evaluation.\n",
    "\n",
    "![k-fold CV](https://www.researchgate.net/profile/Johar-Ashfaque-Aatqb/publication/332370436/figure/fig1/AS:746775958806528@1555056671117/Diagram-of-k-fold-cross-validation-with-k-10-Image-from-Karl-Rosaen-Log.ppm)\n",
    "\n",
    "In summary, k-fold cross-validation makes better use of the dataset than the holdout method with a validation set, since in k-fold cross-validation all data points are being used for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8852735",
   "metadata": {},
   "source": [
    "## 4. Describe the bootstrap sampling method. What is the aim of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a09295",
   "metadata": {},
   "source": [
    "**Bootstrap sample:** A sample taken with replacement from an obeserved data set. In this sampling technique instead of using the same training dataset to fit the individual classifiers in the ensemble, we draw bootstrap samples (random samples with replacement) from the initial training dataset, which is why bagging is also known as bootstrap aggregating.\n",
    "\n",
    "Conceptually, we can imagine the bootstrap as replicating the original sample thousands or millions of times so that we have a hypothetical population that embodies all the knowledge from our original sample (it's just larger). We can then draw samples from this hypothetical population for the purpose of estimating a sampling distribution. In practice it is not necessary to actually replicate the sample a huge number of times. We simply replace each observationafter each draw; that is, we sample with replacement. In this way we effectively create an infinite population in which the probability of an element being drawn remains unchanged from draw to draw.\n",
    "\n",
    "![boot strapping](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/Bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b434a7",
   "metadata": {},
   "source": [
    "## 5. What is the significance of calculating the Kappa value for a classification model? Demonstrate how to measure the Kappa value of a classification model using a sample collection of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511b346",
   "metadata": {},
   "source": [
    "Cohen’s kappa is a metric often used to assess the agreement between two raters. It can also be used to assess the performance of a classification model. For example, if we had two bankers and we asked both to classify 100 customers in two classes for credit rating (i.e., good and bad) based on their creditworthiness, we could then measure the level of their agreement through Cohen’s kappa.\n",
    "\n",
    "Similarly, in the context of a classification model, we could use Cohen’s kappa to compare the machine learning model predictions with the manually established credit ratings.\n",
    "\n",
    "Like many other evaluation metrics, Cohen’s kappa is calculated based on the confusion matrix. However, in contrast to calculating overall accuracy, Cohen’s kappa takes imbalance in class distribution into account and can, therefore, be more complex to interpret.\n",
    "\n",
    "**Cohen's kappa is calculated using following formula:**\n",
    "\n",
    "$$k = \\frac{p_{o}-p_{e}}{1-p_{e}}$$\n",
    "\n",
    "$p_{o}$ is the overall accuracy of the model.\n",
    "$p_{e}$ is the measure of the agreement between the model predictions and the actual class values as if happening by chance.\n",
    "\n",
    "In a binary classification problem $p_{e}$,is the sum of,$p_{e1}$ the probability of the predictions agreeing with actual values of class 1 (“good”) by chance, and, $p_{e2}$the probability of the predictions agreeing with the actual values of class 2 (“bad”) by chance. Assuming that the two classifiers — model predictions and actual class values — are independent, these probabilities,and, $p_{e1}$ and $p_{e2}$are calculated by multiplying the proportion of the actual class and the proportion of the predicted class.\n",
    "\n",
    "Consider To provide more detail about the confusion matrix for this model, 18 out of the 30 customers with a \"bad\" credit rating are detected by the model, leading to a new sensitivity value of 60%. \n",
    "\n",
    "Considering \"bad\" as the positive class, the baseline model assigned 9% of the records (false positives plus true positives) to class \"bad\" and 91% of the records (true negatives plus false negatives) to class \"good.\" Thus, $p_{e}$ is:\n",
    "\n",
    "$$p_{e} = p_{e1} + p_{e2} = p_{e1,target}*p_{e1,pred}+p_{e2,target}*p_{e2,pred} = 0.90*0.91+0.10*0.09 = 0.828$$\n",
    "\n",
    "$$k = \\frac{0.870-0.828}{1-0.828} \\approx 0.244$$\n",
    "\n",
    "In summary:\n",
    "\n",
    "* Cohen’s kappa is more informative than overall accuracy when working with unbalanced data. Keep this in mind when you compare or optimize classification models.\n",
    "* Take a look at the row and column totals in the confusion matrix. Are the distributions of the target/predicted classes similar? If not, the maximum reachable Cohen’s kappa value will be lower.\n",
    "* The same model will give you lower values of Cohen’s kappa for unbalanced than for balanced test data.\n",
    "Cohen’s kappa says little about the expected accuracy of a single prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1adbdd",
   "metadata": {},
   "source": [
    "## 6. Describe the model ensemble method. In machine learning, what part does it play?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9c856",
   "metadata": {},
   "source": [
    "The goal of ensemble methods is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifiers alone. For example, assuming that we collected predictions from 10 experts, ensemble methods would allow us to startegically combine those predictions by the 10 experts to come up with a prediction that was more accurate and robust than the predictions by each individual expert. \n",
    "\n",
    "The most common ensemble methods are:\n",
    "* Majority Voting\n",
    "* Bagging\n",
    "* Boosting\n",
    "* Random Forests\n",
    "* Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2e5ef",
   "metadata": {},
   "source": [
    "## 7. What is a descriptive model's main purpose? Give examples of real-world problems that descriptive models were used to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eaf9c0",
   "metadata": {},
   "source": [
    "A descriptive model will exploit the past data that are stored in databases and provide you with the accurate report. Suppose your task is to optimize the supply chain of a department store, for this task we have purchase and sales data. After analyzing the data, we make assumptions that sales increase during the day just before the weekend. This means that our machine learning model is based on periodicity. So, descriptive analysis helps us understand the deep patterns from the data to uncover all those special features that were overlooked at the initial stage.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Netflix, for example, uses descriptive analytics to find correlations among different movies that subscribers rent and to improve their recommendation engine they used historic sales and customer data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651da63",
   "metadata": {},
   "source": [
    "## 8. Describe how to evaluate a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fca3b8",
   "metadata": {},
   "source": [
    "After training our linear regression model, we should see residual plots for the test and training datasets with a line passing through the x-axis origin. We can also evaluate the model's performance by using Mean Squared Error (MSE) which is a loss function to be minimized to fit the linear regression model. It is given as:\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}\\left ( y^{(i)}-\\hat{y}^{(i)} \\right )^{2}$$\n",
    "\n",
    "Suppose, we used dollar in our dataset, but while using MSE, the scale changed. It will be more intuitive to show the error on the original unit scale(here, dollar instead of dollar squared), which is why we may choose to compute the square root of the MSE, called root mean squared error, or the **mean absolute error (MAE),** which emphasizes incorrect prediction slightly less:\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n}\\left |y^{(i)}-\\hat{y}^{(i)}  \\right |$$\n",
    "\n",
    "When we use the MAE or MSE for comparing models, we need to be aware that these are unbounded in contrast to the classification accuracy. In other words, the interpretation of the MAE and MSE depend on the dataset and feature scaling. It may sometimes be more useful to report the **coefficient of determination** ($R^{2}$), which can be understood as a standardized version of the MSE, for better interpretability of the model's performance. Or, in other words, $R^{2}$ is the fraction of response variance that is captured by the model. The $R^{2}$ value is defined as:\n",
    "\n",
    "$$R^{2} = 1\\ - \\frac{SSE}{SST}$$\n",
    "\n",
    "Here, SSE is the sum of squared errors, which is similar to the MSE but does not include the normalization by sample size n:\n",
    " \n",
    "$$SSE = \\sum_{i=1}^{n}\\left ( y^{(i)}-\\hat{y}^{(i)} \\right )^2$$\n",
    "\n",
    "And SST is the total sum of squares:\n",
    "\n",
    "$$SST = \\sum_{i=1}^{n}\\left ( y^{(i)}-\\mu_{y} \\right )^2$$\n",
    "\n",
    "In other words, SST is simply the variance of the response.\n",
    "\n",
    "now, let's briefly show that $R^{2}$ is indeed just a rescaled version of the MSE:\n",
    "\n",
    "$$R^{2} = 1 - \\frac{SSE}{SST}$$\n",
    "\n",
    "$$R^{2} = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}(y^{i}-\\hat{y}^{(i)})^2}{\\frac{1}{n}\\sum_{i=1}^{n}(y^{i}-\\mu_{y})^2}$$\n",
    "\n",
    "$$R^{2} = 1 - \\frac{MSE}{Var(y)}$$\n",
    "\n",
    "For a more better command on how our model learns and the freedom which we should give to it, we can use Regularization like Lasso(L1), Ridge(L2) and ElasticNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21484942",
   "metadata": {},
   "source": [
    "## 9. Distinguish :\n",
    "\n",
    "1. Descriptive vs. predictive models\n",
    "\n",
    "2. Underfitting vs. overfitting the model\n",
    "\n",
    "3. Bootstrapping vs. cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8b2f3",
   "metadata": {},
   "source": [
    "### **1. Descriptive vs. predictive models:**\n",
    "\n",
    "A descriptive model will exploit the past data that are stored in databases and provide you with the accurate report. Suppose your task is to optimize the supply chain of a department store, for this task we have purchase and sales data. After analyzing the data, we make assumptions that sales increase during the day just before the weekend. This means that our machine learning model is based on periodicity. So, descriptive analysis helps us understand the deep patterns from the data to uncover all those special features that were overlooked at the initial stage.\n",
    "\n",
    "Example: Netflix, for example, uses descriptive analytics to find correlations among different movies that subscribers rent and to improve their recommendation engine they used historic sales and customer data.\n",
    "\n",
    "A predictive model is not fixed; it is validated or revised regularly to incorporate changes in the underlying data. In other words, it’s not a one-and-done prediction. Predictive models make assumptions based on what has happened in the past and what is happening now. If incoming, new data shows changes in what is happening now, the impact on the likely future outcome must be recalculated, too. For example, a software company could model historical sales data against marketing expenditures across multiple regions to create a model for future revenue based on the impact of the marketing spend.\n",
    "\n",
    "Most predictive models work fast and often complete their calculations in real time. That’s why banks and retailers can, for example, calculate the risk of an online mortgage or credit card application and accept or decline the request almost instantly based on that prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10cfb70",
   "metadata": {},
   "source": [
    "### **Underfitting vs. overfitting the model:**\n",
    "\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data.\n",
    "\n",
    "Overfitting is a concept in data science, which occurs when a statistical model fits exactly against its training data. When this happens, the algorithm unfortunately cannot perform accurately against unseen data, defeating its purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f88920",
   "metadata": {},
   "source": [
    "### **3. Bootstrapping vs. cross-validation:**\n",
    "\n",
    "Bootstrapping is any test or metric that relies on random sampling with replacement.It is a method that helps in many situations like validation of a predictive model performance, ensemble methods, estimation of bias and variance of the parameter of a model etc. It works by performing sampling with replacement from the original dataset, and at the same time assuming that the data points that have not been chosen are the test dataset. We can repeat this procedure several times and compute the average score as estimation of our model performance. Also, Bootstrapping is related to the ensemble training methods, because we can build a model using each bootstrap datasets and \"bag\" these models in an ensemble using the majority voting (for classification) or computing the average (for numerical predictions) for all of these models as our final result.\n",
    "\n",
    "Cross validation is a procedure for validating a model's performance, and it is done by splitting the training data into k parts. We assume that the k-1 parts is the training set and use the other part is our test set. We can repeat that k times differently holding out a different part of the data every time. Finally, we take the average of the k scores as our performance estimation. Cross validation can suffer from bias or variance. Increasing the number of splits, the variance will increase too and the bias will decrease. On the other hand, if we decrease the number of splits, the bias will increase and the variance will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49b3a4",
   "metadata": {},
   "source": [
    "## 10. Make quick notes on:\n",
    "\n",
    "            1. LOOCV.\n",
    "\n",
    "            2. F-measurement\n",
    "\n",
    "            3. The width of the silhouette\n",
    "\n",
    "             4. Receiver operating characteristic curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c562a90",
   "metadata": {},
   "source": [
    "### 1. **LOOCV:**\n",
    "\n",
    "LOOCV(Leave One Out Cross-Validation) is a special case of K-fold cross validation method. In LOOCV, we set the number of folds equal to the number of training examples (k = n) so that only one training example is used for testing during each iteration, which is a recommended approach for working with very small datasets.\n",
    "\n",
    "### 2. **F-measurement:**\n",
    "Suppose that, instead of an A/B test, we had a comparison of multiple groups, say A/B/C/D, each with numeric data. The statistical procedure that test for a statistically significant difference among the groups is called analysis of variance, or ANOVA.\n",
    "\n",
    "**Key Terms for ANOVA**\n",
    "\n",
    "* Pairwise comparision: A hypothesis test (e.g., of means) between two groups among multiple groups.\n",
    "* Omnibus test: A single hypothesis test of the overall variance among multiple group means.\n",
    "* Decomposition of Variance: Separation of components contributing to an individual value (e.g., from the overall average, from a treatment mean, and from a residual error).\n",
    "* F-statistic: A standardized statistic that measures the extent to which differences among group means exceed what might be expected in a chance model.\n",
    "* SS: \"Sum of squares\", referring to deviations from some average value.\n",
    "\n",
    "### **3. The width of the silhouette:** \n",
    "\n",
    "It means the distance of x from all the other points in the same cluster. The Average Silhouette Width (ASW) of a clustering is ̄ a ( i ) is the average distance of to points in the cluster to which it was assigned, and is the average distance of to the points in the nearest cluster to which it was not assigned.\n",
    "\n",
    "### **4. Receiver operating characteristic curve:**\n",
    "\n",
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "\n",
    "    * True Positive Rate\n",
    "    * False Positive Rate\n",
    "True Positive Rate (TPR) is a synonym for recall and is therefore defined as follows:\n",
    "\n",
    "$$TPR = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "False Positive Rate (FPR) is defined as follows:\n",
    "\n",
    "$$FPR = \\frac{FP}{FP+TN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc9c447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
